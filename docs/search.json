[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "This data set,originally a CSV file, containing cleaned data from approximately 250,000 individuals. It includes 21 variables derived directly from respondents or calculated based on their responses. The data was collected through a phone survey focused on Behavioral Risk Factors.\nThe primary goal of this data collection and subsequent analysis is to identify the most common predictors for determining whether an individual is more likely to report having diabetes. Key variables of interest include:\n-Sex (Gender): The respondent’s reported gender. -Veggies: Whether the respondent consumes vegetables one or more times per day. -AnyHealthcare: Whether the respondent has any form of health insurance coverage. -GenHealth: A self-reported health status on a scale from 1 (excellent) to 5 (poor). -PhysActivity: Whether the respondent engaged in physical activity (excluding work-related activity) within the past 30 days.\nThe purpose of the exploratory data analysis (EDA) is to ensure the data is clean, prepare it for analysis (e.g., converting numeric responses into meaningful categories for better interpretation), and examine the prevalence of these potential risk factors."
  },
  {
    "objectID": "EDA.html#reading-in-data-and-converting-variables",
    "href": "EDA.html#reading-in-data-and-converting-variables",
    "title": "EDA",
    "section": "Reading in Data and Converting Variables",
    "text": "Reading in Data and Converting Variables\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyr)\n\ndata&lt;-read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndata&lt;- data|&gt;\n  mutate(HighBP=factor(HighBP, levels=c(0,1),labels=c(\"Normal_BP\",\"High_BP\")),\n         HighChol=factor(HighChol, levels=c(0,1),labels=c(\"Normal_Chol\",\"High_Chol\")),\n         CholCheck=as.factor(CholCheck),\n         Smoker=factor(Smoker,levels=c(0,1),labels=c(\"Non_Smoker\",\"Smoker\")),\n         Stroke=factor(Stroke, levels=c(0,1),labels=c(\"No_Stroke\",\"Yes_Stroke\")),\n         Fruits=factor(Fruits,levels=c(0,1),labels=c(\"No_Fruits\",\"Eats_Fruits\")),\n         Veggies=factor(Veggies,levels=c(0,1),labels=c(\"No_Veggies\",\"Eats_Veggies\")),\n         Sex=factor(Sex, level=c(0,1), labels = c(\"Female\",\"Male\")),\n         GenHlth=factor(GenHlth,levels=c(1,2,3,4,5), \n                        labels=c(\"Excellent\",\"Very_Good\",\"Good\",\"Fair\",\"Poor\")),\n         Age=factor(Age, levels=c(1,2,3,4,5,6,7,8,9,10,11,12,13),\n                    labels=c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\n                            \"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80+\" )),\n         Education=factor(Education,levels=c(1,2,3,4,5,6),labels=c(\"None-K\",\"Grades 1-8\",\n                                                                   \"Grades 9-11\",\"HS Diploma\",\"Some College\",\"Bachelor's Degree or Higher\")),\n         Income=factor(Income,levels=c(1,2,3,4,5,6,7,8),labels=c(\"&lt;10K\",\"&lt;15K\",\n          \"&lt;20K\",\"&lt;25K\",\"&lt;35K\",\"&lt;50K\",\n          \"&lt;75K\",\"&gt;=75K\")),\n         Diabetes_binary=factor(Diabetes_binary,levels=c(0,1),labels=c(\"Not_Diabetic\",\"Diabetic\")),\n         HeartDiseaseorAttack=factor(HeartDiseaseorAttack,levels=c(0,1),labels=c(\"No_Heart Disease\",\"Heart_Disease\")),\n         PhysActivity=factor(PhysActivity,levels=c(0,1),labels=c(\"No_Physical Activity\",\"Physical_Activity\")),\n         HvyAlcoholConsump=factor(HvyAlcoholConsump,levels=c(0,1),labels=c(\"Not_Heavy Drinker\",\"Heavy_Drinker\")),\n         AnyHealthcare=factor(AnyHealthcare,levels=c(0,1),labels=c(\"No_Health_Coverage\",\"Yes_Health_Coverage\")),\n         NoDocbcCost=factor(NoDocbcCost,levels=c(0,1),labels=c(\"Cost_Issue\",\"Cost_Not_Issue\")),\n         DiffWalk=factor(DiffWalk,levels=c(0,1),labels=c(\"No_Difficulty_Walking\",\"Difficulty_Walking\"))\n         )\n\nstr(data)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"Not_Diabetic\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"Normal_BP\",\"High_BP\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"Normal_Chol\",..: 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"Non_Smoker\",\"Smoker\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No_Stroke\",\"Yes_Stroke\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No_Heart Disease\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No_Physical Activity\",..: 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"No_Fruits\",\"Eats_Fruits\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"No_Veggies\",\"Eats_Veggies\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"Not_Heavy Drinker\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"No_Health_Coverage\",..: 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"Cost_Issue\",\"Cost_Not_Issue\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"Excellent\",\"Very_Good\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"No_Difficulty_Walking\",..: 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"18-24\",\"25-29\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"None-K\",\"Grades 1-8\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"&lt;10K\",\"&lt;15K\",..: 3 1 8 6 4 8 7 4 1 3 ..."
  },
  {
    "objectID": "EDA.html#basic-summaries-and-rates-of-missing",
    "href": "EDA.html#basic-summaries-and-rates-of-missing",
    "title": "EDA",
    "section": "Basic Summaries and Rates of Missing",
    "text": "Basic Summaries and Rates of Missing\n\nsum_na&lt;-function(column){\n  sum(is.na(column))\n}\n\nna_counts&lt;-data|&gt;\n  summarize(across(everything(),sum_na))\n\nprint(na_counts)\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      0        0         0   0      0      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       0                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             0           0       0        0        0        0   0   0\n  Education Income\n1         0      0\n\nsummaries&lt;- data|&gt;\n  summarize(across(where(is.numeric),\n                   list(\"mean\"=mean,\n                         \"median\"=median,\n                          \"min\"=min,\n                          \"max\"=max,\n                           \"sd\"=sd,\n                           \"iqr\"=IQR),\n                    .names=\"{.fn}_{.col}\"))\nprint(summaries)\n\n  mean_BMI median_BMI min_BMI max_BMI   sd_BMI iqr_BMI mean_MentHlth\n1 28.38236         27      12      98 6.608694       7      3.184772\n  median_MentHlth min_MentHlth max_MentHlth sd_MentHlth iqr_MentHlth\n1               0            0           30    7.412847            2\n  mean_PhysHlth median_PhysHlth min_PhysHlth max_PhysHlth sd_PhysHlth\n1      4.242081               0            0           30    8.717951\n  iqr_PhysHlth\n1            3\n\n\n\ntable(data$Sex,data$Diabetes_binary)\n\n        \n         Not_Diabetic Diabetic\n  Female       123563    18411\n  Male          94771    16935\n\ntable(data$Sex,data$Diabetes_binary,data$Age)\n\n, ,  = 18-24\n\n        \n         Not_Diabetic Diabetic\n  Female         2700       45\n  Male           2922       33\n\n, ,  = 25-29\n\n        \n         Not_Diabetic Diabetic\n  Female         3902       89\n  Male           3556       51\n\n, ,  = 30-34\n\n        \n         Not_Diabetic Diabetic\n  Female         5871      191\n  Male           4938      123\n\n, ,  = 35-39\n\n        \n         Not_Diabetic Diabetic\n  Female         7359      366\n  Male           5838      260\n\n, ,  = 40-44\n\n        \n         Not_Diabetic Diabetic\n  Female         8560      576\n  Male           6546      475\n\n, ,  = 45-49\n\n        \n         Not_Diabetic Diabetic\n  Female        10026      902\n  Male           8051      840\n\n, ,  = 50-54\n\n        \n         Not_Diabetic Diabetic\n  Female        13163     1642\n  Male          10063     1446\n\n, ,  = 55-59\n\n        \n         Not_Diabetic Diabetic\n  Female        15214     2255\n  Male          11355     2008\n\n, ,  = 60-64\n\n        \n         Not_Diabetic Diabetic\n  Female        15303     2968\n  Male          12208     2765\n\n, ,  = 65-69\n\n        \n         Not_Diabetic Diabetic\n  Female        14493     3250\n  Male          11143     3308\n\n, ,  = 70-74\n\n        \n         Not_Diabetic Diabetic\n  Female        10606     2553\n  Male           7786     2588\n\n, ,  = 75-79\n\n        \n         Not_Diabetic Diabetic\n  Female         7584     1834\n  Male           4993     1569\n\n, ,  = 80+\n\n        \n         Not_Diabetic Diabetic\n  Female         8782     1740\n  Male           5372     1469\n\ntable(data$Education,data$HighBP)\n\n                             \n                              Normal_BP High_BP\n  None-K                             86      88\n  Grades 1-8                       1654    2389\n  Grades 9-11                      3975    5503\n  HS Diploma                      31030   31720\n  Some College                    39019   30891\n  Bachelor's Degree or Higher     69087   38238\n\ntable(data$Education,data$HighChol)\n\n                             \n                              Normal_Chol High_Chol\n  None-K                               87        87\n  Grades 1-8                         1878      2165\n  Grades 9-11                        4741      4737\n  HS Diploma                        33789     28961\n  Some College                      40223     29687\n  Bachelor's Degree or Higher       65371     41954\n\ntable(data$Income)\n\n\n &lt;10K  &lt;15K  &lt;20K  &lt;25K  &lt;35K  &lt;50K  &lt;75K &gt;=75K \n 9811 11783 15994 20135 25883 36470 43219 90385 \n\ndata2&lt;-data|&gt;\n  filter( Income != \"&gt;=75K\" )\n\ntable(data2$Fruits,data2$Veggies,data2$Diabetes_binary)\n\n, ,  = Not_Diabetic\n\n             \n              No_Veggies Eats_Veggies\n  No_Fruits        18057        33201\n  Eats_Fruits      10772        73114\n\n, ,  = Diabetic\n\n             \n              No_Veggies Eats_Veggies\n  No_Fruits         4598         7293\n  Eats_Fruits       2899        13361\n\n\n\nggplot(data=data,aes(x=Sex,fill=Diabetes_binary))+\n  geom_bar(position=\"fill\")+\n  labs(x=\"Sex\")+\n  scale_fill_discrete(\"Diabetes Status\")\n\n\n\n\n\n\n\nggplot(data=data|&gt;group_by(Income),aes(x=Income,fill=Diabetes_binary))+\n  geom_bar(position=\"fill\")+\n  labs(x=\"Income\")+\n  scale_fill_discrete(\"Diabetes Status\")\n\n\n\n\n\n\n\n\n\n# Create a grouped boxplot using ggplot2\nggplot(data = data, aes(x = Education, y = BMI,\n                        fill = factor(Income))) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(fill = \"Income\")"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(baguette)\nlibrary(parsnip)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nThe following objects are masked from 'package:scales':\n\n    alpha, rescale\n\nlibrary(tidymodels)\nlibrary(stats)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(parsnip)\nlibrary(tune)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(baguette)\nlibrary(ranger)\nlibrary(rpart.plot)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift"
  },
  {
    "objectID": "Modeling.html#libraries",
    "href": "Modeling.html#libraries",
    "title": "Modeling",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(baguette)\nlibrary(parsnip)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nThe following objects are masked from 'package:scales':\n\n    alpha, rescale\n\nlibrary(tidymodels)\nlibrary(stats)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(parsnip)\nlibrary(tune)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(baguette)\nlibrary(ranger)\nlibrary(rpart.plot)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift"
  },
  {
    "objectID": "Modeling.html#setting-up-data",
    "href": "Modeling.html#setting-up-data",
    "title": "Modeling",
    "section": "Setting up Data",
    "text": "Setting up Data\n\ndata&lt;-read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndata&lt;- data|&gt;\n  mutate(HighBP=factor(HighBP, levels=c(0,1),labels=c(\"Normal_BP\",\"High_BP\")),\n         HighChol=factor(HighChol, levels=c(0,1),labels=c(\"Normal_Chol\",\"High_Chol\")),\n         CholCheck=as.factor(CholCheck),\n         Smoker=factor(Smoker,levels=c(0,1),labels=c(\"Non_Smoker\",\"Smoker\")),\n         Stroke=factor(Stroke, levels=c(0,1),labels=c(\"No_Stroke\",\"Yes_Stroke\")),\n         Fruits=factor(Fruits,levels=c(0,1),labels=c(\"No_Fruits\",\"Eats_Fruits\")),\n         Veggies=factor(Veggies,levels=c(0,1),labels=c(\"No_Veggies\",\"Eats_Veggies\")),\n         Sex=factor(Sex, level=c(0,1), labels = c(\"Female\",\"Male\")),\n         GenHlth=factor(GenHlth,levels=c(1,2,3,4,5), \n                        labels=c(\"Excellent\",\"Very_Good\",\"Good\",\"Fair\",\"Poor\")),\n         Age=factor(Age, levels=c(1,2,3,4,5,6,7,8,9,10,11,12,13),\n                    labels=c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\n                            \"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80+\" )),\n         Education=factor(Education,levels=c(1,2,3,4,5,6),labels=c(\"None-K\",\"Grades 1-8\",\n                                                                   \"Grades 9-11\",\"HS Diploma\",\"Some College\",\"Bachelor's Degree or Higher\")),\n         Income=factor(Income,levels=c(1,2,3,4,5,6,7,8),labels=c(\"&lt;10K\",\"&lt;15K\",\n          \"&lt;20K\",\"&lt;25K\",\"&lt;35K\",\"&lt;50K\",\n          \"&lt;75K\",\"&gt;=75K\")),\n         Diabetes_binary=factor(Diabetes_binary,levels=c(0,1),labels=c(\"Not_Diabetic\",\"Diabetic\")),\n         HeartDiseaseorAttack=factor(HeartDiseaseorAttack,levels=c(0,1),labels=c(\"No_Heart Disease\",\"Heart_Disease\")),\n         PhysActivity=factor(PhysActivity,levels=c(0,1),labels=c(\"No_Physical Activity\",\"Physical_Activity\")),\n         HvyAlcoholConsump=factor(HvyAlcoholConsump,levels=c(0,1),labels=c(\"Not_Heavy Drinker\",\"Heavy_Drinker\")),\n         AnyHealthcare=factor(AnyHealthcare,levels=c(0,1),labels=c(\"No_Health_Coverage\",\"Yes_Health_Coverage\")),\n         NoDocbcCost=factor(NoDocbcCost,levels=c(0,1),labels=c(\"Cost_Issue\",\"Cost_Not_Issue\")),\n         DiffWalk=factor(DiffWalk,levels=c(0,1),labels=c(\"No_Difficulty_Walking\",\"Difficulty_Walking\"))\n         )\n\n\nset.seed(1234)\n\nsplit_data&lt;-initial_split(data,prop=0.7,strata = Diabetes_binary)\ntrain_data&lt;-training(split_data)\ntest_data&lt;-testing(split_data)\ncv_fold_data&lt;-vfold_cv(train_data,v=5,strata=Diabetes_binary)"
  },
  {
    "objectID": "Modeling.html#random-forest-model",
    "href": "Modeling.html#random-forest-model",
    "title": "Modeling",
    "section": "Random Forest Model",
    "text": "Random Forest Model\nIn this section, we will train a Random Forest model, an ensemble learning method that combines multiple randomized decision trees to produce robust predictions. The model leverages bootstrapping, a resampling technique with replacement, to generate diverse training subsets, enhancing the model’s robustness. Additionally, during tree construction, the algorithm randomly selects a subset of predictor variables for each split, reducing correlation among the trees and improving predictive accuracy. The final prediction is determined by aggregating the votes from all individual decision trees, with the majority vote dictating the predicted class label.\n\ndata2&lt;-data|&gt;\n  select(Sex,Veggies,AnyHealthcare,GenHlth,PhysActivity)\n\n# Creating a recipe \nrec1&lt;-recipe(Diabetes_binary~Sex+Veggies+AnyHealthcare+GenHlth+PhysActivity,data=train_data)|&gt;\n   step_dummy(Sex,Veggies,GenHlth,AnyHealthcare,PhysActivity)\n\n\n# Creating a Random Forest Model with 100 trees. \nrf_spec&lt;-rand_forest(mtry=tune(),trees = 100)|&gt;\n  set_engine(\"ranger\", importance = \"impurity\")|&gt;\n  set_mode(\"classification\")\n\n\n#Creating the workflow based on the recipe and model from above. \nrf_wkf&lt;-workflow()|&gt;\n  add_recipe(rec1)|&gt;\n  add_model(rf_spec)\n\n\n#Creating predictors that the model will use base on how many predictors my model has. \nmtry_vals &lt;- grid_regular(mtry(range = c(1, ncol(data2) - 1)), levels = 5)\n\n\n#Training the model with the cross validation data set using a log loss metric.\nrf_fit&lt;-rf_wkf|&gt;\n  tune_grid(resamples=cv_fold_data,\n            grid=mtry_vals,\n            metrics=metric_set(mn_log_loss))\n\n\n#Looking at the different models that have different number of predictor variables and their log loss metric. \nrf_fit|&gt;\n  collect_metrics()|&gt;\n  filter(.metric==\"mn_log_loss\")|&gt;\n  arrange(mean)\n\n# A tibble: 4 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     4 mn_log_loss binary     0.358     5 0.000388 Preprocessor1_Model4\n2     3 mn_log_loss binary     0.359     5 0.000323 Preprocessor1_Model3\n3     2 mn_log_loss binary     0.364     5 0.000557 Preprocessor1_Model2\n4     1 mn_log_loss binary     0.377     5 0.000474 Preprocessor1_Model1\n\n\n\n# Selection the best model.\nrf_best_params&lt;-select_best(rf_fit,metric = \"mn_log_loss\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     4 Preprocessor1_Model4\n\n\n\n# Finalizing the work flow with the best model.\nrf_final_wkf&lt;-rf_wkf|&gt;\n  finalize_workflow(rf_best_params)\n\n\n#Refitting the model \nrf_final_fit&lt;-rf_final_wkf|&gt;\n  last_fit(split_data,metrics=metric_set(mn_log_loss))\n\n\n#Finding the best model\nrf_final_fit|&gt;collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.357 Preprocessor1_Model1\n\n\n\nrf_model &lt;- extract_fit_parsnip(rf_final_fit)\n\n\nimportance &lt;- ranger::importance(rf_model$fit)\n\n\ncoef_table &lt;- tibble(\n  variable = names(importance),\n  importance = importance)|&gt;\n  arrange(desc(importance))\nprint(coef_table)\n\n# A tibble: 8 × 2\n  variable                          importance\n  &lt;chr&gt;                                  &lt;dbl&gt;\n1 GenHlth_Fair                          1431. \n2 GenHlth_Poor                          1128. \n3 GenHlth_Good                           746. \n4 GenHlth_Very_Good                      288. \n5 PhysActivity_Physical_Activity         170. \n6 Sex_Male                                51.1\n7 AnyHealthcare_Yes_Health_Coverage       45.7\n8 Veggies_Eats_Veggies                    29.8"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\n\ntree_mod&lt;-decision_tree(tree_depth = tune(),\n                        min_n = 5,\n                        cost_complexity = tune()\n                        )|&gt;\n  set_engine(\"rpart\")|&gt;\n  set_mode(\"classification\")\n\ntree_wkf&lt;-workflow()|&gt;\n  add_recipe(rec1)|&gt;\n  add_model(tree_mod)\n\ntemp&lt;-tree_wkf|&gt;\n  tune_grid(resamples = cv_fold_data,metrics = metric_set(mn_log_loss))\n  temp|&gt;\n    collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        3.68e- 3          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2        5.69e-10          9 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3        5.06e- 9          6 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4        3.87e- 5          3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5        2.38e- 7         12 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6        1.02e- 3         13 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 7        9.14e- 7         14 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 8        3.68e- 8          2 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 9        1.09e- 5          8 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10        4.29e- 2          8 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n\n  tree_grid&lt;-grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels=c(5,5))\n  tree_fits&lt;-tree_wkf|&gt;\n    tune_grid(resamples=cv_fold_data,metrics=metric_set(mn_log_loss),\n              grid=tree_grid)\n  \n  tree_fits\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits                 id    .metrics          .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [142059/35516]&gt; Fold1 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [142059/35516]&gt; Fold2 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [142060/35515]&gt; Fold3 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [142061/35514]&gt; Fold4 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [142061/35514]&gt; Fold5 &lt;tibble [25 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n  tree_fits|&gt;\n    collect_metrics()|&gt;\n    filter(.metric==\"mn_log_loss\")|&gt;\n    arrange(mean)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10    0.1                   4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n# ℹ 15 more rows\n\ntree_best_params&lt;-select_best(tree_fits, metric=\"mn_log_loss\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          1 Preprocessor1_Model01\n\ntree_final_wkf&lt;-tree_wkf|&gt;\n  finalize_workflow(tree_best_params)\n\ntree_final_fit&lt;-tree_final_wkf|&gt;\n  last_fit(split_data,metrics=metric_set(mn_log_loss))\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177575/76105]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\ntree_final_fit|&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.404 Preprocessor1_Model1\n\ntree_final_model&lt;-extract_workflow(tree_final_fit)\ntree_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 177575 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 177575 24742 Not_Diabetic (0.8606673 0.1393327) *\n\ntree_final_model %&gt;%\n  extract_fit_engine () %&gt;%\n  rpart.plot::rpart.plot(roundint=FALSE)"
  },
  {
    "objectID": "Modeling.html#final-and-best-model",
    "href": "Modeling.html#final-and-best-model",
    "title": "Modeling",
    "section": "Final and Best Model",
    "text": "Final and Best Model\n\nbest&lt;-rf_wkf|&gt;\n  finalize_workflow(rf_best_params)|&gt;\n  fit(train_data)\nbest\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4L,      x), num.trees = ~100, importance = ~\"impurity\", num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  100 \nSample size:                      177575 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1087852"
  },
  {
    "objectID": "EDA.html#click-here-for-the-modeling-page",
    "href": "EDA.html#click-here-for-the-modeling-page",
    "title": "EDA",
    "section": "Click here for the modeling page",
    "text": "Click here for the modeling page"
  },
  {
    "objectID": "EDA.html#navigation",
    "href": "EDA.html#navigation",
    "title": "EDA",
    "section": "Navigation",
    "text": "Navigation\nNavigate to the Modeling Page for more details about the modeling process."
  },
  {
    "objectID": "EDA.html#introduction-to-the-data-and-my-analysis",
    "href": "EDA.html#introduction-to-the-data-and-my-analysis",
    "title": "EDA",
    "section": "",
    "text": "This data set,originally a CSV file, containing cleaned data from approximately 250,000 individuals. It includes 21 variables derived directly from respondents or calculated based on their responses. The data was collected through a phone survey focused on Behavioral Risk Factors.\nThe primary goal of this data collection and subsequent analysis is to identify the most common predictors for determining whether an individual is more likely to report having diabetes. Key variables of interest include:\n-Sex (Gender): The respondent’s reported gender. -Veggies: Whether the respondent consumes vegetables one or more times per day. -AnyHealthcare: Whether the respondent has any form of health insurance coverage. -GenHealth: A self-reported health status on a scale from 1 (excellent) to 5 (poor). -PhysActivity: Whether the respondent engaged in physical activity (excluding work-related activity) within the past 30 days.\nThe purpose of the exploratory data analysis (EDA) is to ensure the data is clean, prepare it for analysis (e.g., converting numeric responses into meaningful categories for better interpretation), and examine the prevalence of these potential risk factors."
  },
  {
    "objectID": "EDA.html#reading-in-data-and-converting-variables.",
    "href": "EDA.html#reading-in-data-and-converting-variables.",
    "title": "EDA",
    "section": "Reading in Data and Converting Variables.",
    "text": "Reading in Data and Converting Variables.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyr)\nset.seed(1234)\n\ndata&lt;-read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndata&lt;- data|&gt;\n  mutate(HighBP=factor(HighBP, levels=c(0,1),labels=c(\"Normal_BP\",\"High_BP\")),\n         HighChol=factor(HighChol, levels=c(0,1),labels=c(\"Normal_Chol\",\"High_Chol\")),\n         CholCheck=as.factor(CholCheck),\n         Smoker=factor(Smoker,levels=c(0,1),labels=c(\"Non_Smoker\",\"Smoker\")),\n         Stroke=factor(Stroke, levels=c(0,1),labels=c(\"No_Stroke\",\"Yes_Stroke\")),\n         Fruits=factor(Fruits,levels=c(0,1),labels=c(\"No_Fruits\",\"Eats_Fruits\")),\n         Veggies=factor(Veggies,levels=c(0,1),labels=c(\"No_Veggies\",\"Eats_Veggies\")),\n         Sex=factor(Sex, level=c(0,1), labels = c(\"Female\",\"Male\")),\n         GenHlth=factor(GenHlth,levels=c(1,2,3,4,5), \n                        labels=c(\"Excellent\",\"Very_Good\",\"Good\",\"Fair\",\"Poor\")),\n         Age=factor(Age, levels=c(1,2,3,4,5,6,7,8,9,10,11,12,13),\n                    labels=c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\n                            \"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80+\" )),\n         Education=factor(Education,levels=c(1,2,3,4,5,6),labels=c(\"None-K\",\"Grades 1-8\",\n                                                                   \"Grades 9-11\",\"HS Diploma\",\"Some College\",\"Bachelor's Degree or Higher\")),\n         Income=factor(Income,levels=c(1,2,3,4,5,6,7,8),labels=c(\"&lt;10K\",\"&lt;15K\",\n          \"&lt;20K\",\"&lt;25K\",\"&lt;35K\",\"&lt;50K\",\n          \"&lt;75K\",\"&gt;=75K\")),\n         Diabetes_binary=factor(Diabetes_binary,levels=c(0,1),labels=c(\"Not_Diabetic\",\"Diabetic\")),\n         HeartDiseaseorAttack=factor(HeartDiseaseorAttack,levels=c(0,1),labels=c(\"No_Heart Disease\",\"Heart_Disease\")),\n         PhysActivity=factor(PhysActivity,levels=c(0,1),labels=c(\"No_Physical Activity\",\"Physical_Activity\")),\n         HvyAlcoholConsump=factor(HvyAlcoholConsump,levels=c(0,1),labels=c(\"Not_Heavy Drinker\",\"Heavy_Drinker\")),\n         AnyHealthcare=factor(AnyHealthcare,levels=c(0,1),labels=c(\"No_Health_Coverage\",\"Yes_Health_Coverage\")),\n         NoDocbcCost=factor(NoDocbcCost,levels=c(0,1),labels=c(\"Cost_Issue\",\"Cost_Not_Issue\")),\n         DiffWalk=factor(DiffWalk,levels=c(0,1),labels=c(\"No_Difficulty_Walking\",\"Difficulty_Walking\"))\n         )\n\n#Displaying the data for you to see the factor levels, and which variables are numeric.\nstr(data)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"Not_Diabetic\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"Normal_BP\",\"High_BP\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"Normal_Chol\",..: 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"Non_Smoker\",\"Smoker\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No_Stroke\",\"Yes_Stroke\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No_Heart Disease\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No_Physical Activity\",..: 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"No_Fruits\",\"Eats_Fruits\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"No_Veggies\",\"Eats_Veggies\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"Not_Heavy Drinker\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"No_Health_Coverage\",..: 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"Cost_Issue\",\"Cost_Not_Issue\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"Excellent\",\"Very_Good\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"No_Difficulty_Walking\",..: 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"18-24\",\"25-29\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"None-K\",\"Grades 1-8\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"&lt;10K\",\"&lt;15K\",..: 3 1 8 6 4 8 7 4 1 3 ..."
  },
  {
    "objectID": "EDA.html#confirming-the-data-doesnt-have-any-missing-observations.",
    "href": "EDA.html#confirming-the-data-doesnt-have-any-missing-observations.",
    "title": "EDA",
    "section": "Confirming the data doesn’t have any missing observations.",
    "text": "Confirming the data doesn’t have any missing observations.\n\nset.seed(1234)\nsum_na&lt;-function(column){\n  sum(is.na(column))\n}\n\nna_counts&lt;-data|&gt;\n  summarize(across(everything(),sum_na))\n\nprint(na_counts)\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      0        0         0   0      0      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       0                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             0           0       0        0        0        0   0   0\n  Education Income\n1         0      0"
  },
  {
    "objectID": "EDA.html#in-this-section-i-aimed-to-explore-basic-descriptive-statistics-including-the-minimum-maximum-mean-median-and-spread-for-the-numeric-variables.-the-variables-analyzed-include",
    "href": "EDA.html#in-this-section-i-aimed-to-explore-basic-descriptive-statistics-including-the-minimum-maximum-mean-median-and-spread-for-the-numeric-variables.-the-variables-analyzed-include",
    "title": "EDA",
    "section": "In this section, I aimed to explore basic descriptive statistics, including the minimum, maximum, mean, median, and spread, for the numeric variables. The variables analyzed include:",
    "text": "In this section, I aimed to explore basic descriptive statistics, including the minimum, maximum, mean, median, and spread, for the numeric variables. The variables analyzed include:\nBMI (Body Mass Index): A measure of body weight relative to height. MentHlth: The number of days in the past month affected by stress, depression, or other mental health issues. PhysHlth: The number of days in the past month affected by physical illness or injury. Mental and physical well-being are critical components of overall health. In this analysis, I observed that the mean and median for both MentHlth and PhysHlth are 0, with an interquartile range (IQR) of 0. This suggests that, for the majority of respondents in this dataset, mental and physical health issues have minimal reported impact, indicating these factors may not significantly affect this surveyed group.\n\nsummaries&lt;- data|&gt;\n  summarize(across(where(is.numeric),\n                   list(\"mean\"=mean,\n                         \"median\"=median,\n                          \"min\"=min,\n                          \"max\"=max,\n                           \"sd\"=sd,\n                           \"iqr\"=IQR),\n                    .names=\"{.fn}_{.col}\"))\nprint(summaries)\n\n  mean_BMI median_BMI min_BMI max_BMI   sd_BMI iqr_BMI mean_MentHlth\n1 28.38236         27      12      98 6.608694       7      3.184772\n  median_MentHlth min_MentHlth max_MentHlth sd_MentHlth iqr_MentHlth\n1               0            0           30    7.412847            2\n  mean_PhysHlth median_PhysHlth min_PhysHlth max_PhysHlth sd_PhysHlth\n1      4.242081               0            0           30    8.717951\n  iqr_PhysHlth\n1            3"
  },
  {
    "objectID": "EDA.html#to-begin-i-analyzed-the-distribution-of-diabetes-status-by-gender.-while-the-dataset-contains-a-higher-number-of-women-compared-to-men-the-prevalence-of-diabetes-appears-to-be-similar-between-the-two-groups.",
    "href": "EDA.html#to-begin-i-analyzed-the-distribution-of-diabetes-status-by-gender.-while-the-dataset-contains-a-higher-number-of-women-compared-to-men-the-prevalence-of-diabetes-appears-to-be-similar-between-the-two-groups.",
    "title": "EDA",
    "section": "To begin, I analyzed the distribution of diabetes status by gender. While the dataset contains a higher number of women compared to men, the prevalence of diabetes appears to be similar between the two groups.",
    "text": "To begin, I analyzed the distribution of diabetes status by gender. While the dataset contains a higher number of women compared to men, the prevalence of diabetes appears to be similar between the two groups.\n\ntable(data$Sex,data$Diabetes_binary)\n\n        \n         Not_Diabetic Diabetic\n  Female       123563    18411\n  Male          94771    16935\n\nggplot(data=data,aes(x=Sex,fill=Diabetes_binary))+\n  geom_bar()+\n  labs(x=\"Sex\")+\n  scale_fill_discrete(\"Diagnosis\")+\n  coord_flip()"
  },
  {
    "objectID": "EDA.html#pie-chart",
    "href": "EDA.html#pie-chart",
    "title": "EDA",
    "section": "Pie Chart",
    "text": "Pie Chart\nI analyzed the relationship between individuals’ self-reported general health levels and their diabetes status. My initial hunch was that individuals reporting excellent health would have lower rates of diabetes, while those reporting poor health would exhibit the highest rates. Given that the average diabetes prevalence in the United States is approximately 20%, I anticipated this trend to hold across the dataset.\nUpon examining the data, an interesting pattern emerged: individuals reporting “Good” and “Fair” general health had unexpectedly higher rates of diabetes, while those reporting “Poor” health had lower-than-expected rates. Specifically, a pie chart visualization revealed that approximately 38.1% of individuals reporting “Good” general health had diabetes, followed by 27.7% of those reporting “Fair” health. In contrast, only 13% of individuals who reported “Poor” general health were diabetic.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nset.seed(1234)\n#Table with counts of reported general health \ntable(data$GenHlth,data$Diabetes_binary)\n\n           \n            Not_Diabetic Diabetic\n  Excellent        44159     1140\n  Very_Good        82703     6381\n  Good             62189    13457\n  Fair             21780     9790\n  Poor              7503     4578\n\n# Filter the data for individuals with diabetes\ndata_with_diabetes &lt;- data |&gt;\n  filter(Diabetes_binary == \"Diabetic\")\n\n# Calculate the percentages of each health status\nhealth_status_percent &lt;- data_with_diabetes |&gt;\n  group_by(GenHlth) |&gt;\n  summarise(Count = n()) |&gt;\n  mutate(Percentage = round(Count / sum(Count) * 100, 1))\n\n\nggplot(health_status_percent, aes(x = \"\", y = Percentage, fill = GenHlth)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = paste0(Percentage, \"%\")), position = position_stack(vjust = 0.5)) +\n  labs(title = \"General Health Status for People with Diabetes\")"
  },
  {
    "objectID": "EDA.html#next-i-explored-the-relationship-between-lifestyle-factors-specifically-daily-vegetable-consumption-and-physical-activity-and-their-association-with-elevated-bmi-and-the-prevalence-of-diabetes.-i-focused-on-these-behaviors-rather-than-metrics-such-as-high-cholesterol-or-blood-pressure-as-my-interest-was-in-examining-individual-habits-rather-than-physiological-indicators.-the-analysis-revealed-that-among-participants-with-diabetes-who-reported-not-consuming-vegetables-daily-approximately-75-had-a-bmi-exceeding-the-median-bmi-of-those-who-consumed-vegetables-daily-and-were-not-diabetic.",
    "href": "EDA.html#next-i-explored-the-relationship-between-lifestyle-factors-specifically-daily-vegetable-consumption-and-physical-activity-and-their-association-with-elevated-bmi-and-the-prevalence-of-diabetes.-i-focused-on-these-behaviors-rather-than-metrics-such-as-high-cholesterol-or-blood-pressure-as-my-interest-was-in-examining-individual-habits-rather-than-physiological-indicators.-the-analysis-revealed-that-among-participants-with-diabetes-who-reported-not-consuming-vegetables-daily-approximately-75-had-a-bmi-exceeding-the-median-bmi-of-those-who-consumed-vegetables-daily-and-were-not-diabetic.",
    "title": "EDA",
    "section": "Next, I explored the relationship between lifestyle factors, specifically daily vegetable consumption and physical activity, and their association with elevated BMI and the prevalence of diabetes. I focused on these behaviors rather than metrics such as high cholesterol or blood pressure, as my interest was in examining individual habits rather than physiological indicators. The analysis revealed that among participants with diabetes who reported not consuming vegetables daily, approximately 75% had a BMI exceeding the median BMI of those who consumed vegetables daily and were not diabetic.",
    "text": "Next, I explored the relationship between lifestyle factors, specifically daily vegetable consumption and physical activity, and their association with elevated BMI and the prevalence of diabetes. I focused on these behaviors rather than metrics such as high cholesterol or blood pressure, as my interest was in examining individual habits rather than physiological indicators. The analysis revealed that among participants with diabetes who reported not consuming vegetables daily, approximately 75% had a BMI exceeding the median BMI of those who consumed vegetables daily and were not diabetic.\n\n# Create a grouped boxplot using ggplot2\nggplot(data = data, aes(x = Veggies, y = BMI,\n                        fill = factor(Diabetes_binary))) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(fill = \"Diabetes Status\")"
  },
  {
    "objectID": "EDA.html#a-similar-trend-is-observed-with-physical-activity-as-with-vegetable-consumption.",
    "href": "EDA.html#a-similar-trend-is-observed-with-physical-activity-as-with-vegetable-consumption.",
    "title": "EDA",
    "section": "A similar trend is observed with physical activity as with vegetable consumption.",
    "text": "A similar trend is observed with physical activity as with vegetable consumption.\n\nset.seed(1234)\nggplot(data = data, aes(x = PhysActivity, y = BMI,\n                        fill = factor(Diabetes_binary))) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(fill = \"Diabetes Status\")"
  },
  {
    "objectID": "EDA.html#having-examined-the-predictor-variables-i-consider-most-significant-i-will-now-incorporate-them-into-a-classification-tree-and-random-forest-model.-click-the-link-below-to-explore-this-analysis-further",
    "href": "EDA.html#having-examined-the-predictor-variables-i-consider-most-significant-i-will-now-incorporate-them-into-a-classification-tree-and-random-forest-model.-click-the-link-below-to-explore-this-analysis-further",
    "title": "EDA",
    "section": "Having examined the predictor variables I consider most significant, I will now incorporate them into a classification tree and random forest model. Click the link below to explore this analysis further",
    "text": "Having examined the predictor variables I consider most significant, I will now incorporate them into a classification tree and random forest model. Click the link below to explore this analysis further\nNavigate to the Modeling Page for more details about the modeling"
  },
  {
    "objectID": "EDA.html#summaries",
    "href": "EDA.html#summaries",
    "title": "EDA",
    "section": "Summaries",
    "text": "Summaries\nIn this section, I aimed to explore basic descriptive statistics, including the minimum, maximum, mean, median, and spread, for the numeric variables. The variables analyzed include:\nBMI (Body Mass Index): A measure of body weight relative to height. MentHlth: The number of days in the past month affected by stress, depression, or other mental health issues. PhysHlth: The number of days in the past month affected by physical illness or injury. Mental and physical well-being are critical components of overall health. In this analysis, I observed that the mean and median for both MentHlth and PhysHlth are 0, with an interquartile range (IQR) of 0. This suggests that, for the majority of respondents in this dataset, mental and physical health issues have minimal reported impact, indicating these factors may not significantly affect this surveyed group.\n\nset.seed(1234)\nsummaries&lt;- data|&gt;\n  summarize(across(where(is.numeric),\n                   list(\"mean\"=mean,\n                         \"median\"=median,\n                          \"min\"=min,\n                          \"max\"=max,\n                           \"sd\"=sd,\n                           \"iqr\"=IQR),\n                    .names=\"{.fn}_{.col}\"))\nprint(summaries)\n\n  mean_BMI median_BMI min_BMI max_BMI   sd_BMI iqr_BMI mean_MentHlth\n1 28.38236         27      12      98 6.608694       7      3.184772\n  median_MentHlth min_MentHlth max_MentHlth sd_MentHlth iqr_MentHlth\n1               0            0           30    7.412847            2\n  mean_PhysHlth median_PhysHlth min_PhysHlth max_PhysHlth sd_PhysHlth\n1      4.242081               0            0           30    8.717951\n  iqr_PhysHlth\n1            3"
  },
  {
    "objectID": "EDA.html#graphical-summaries",
    "href": "EDA.html#graphical-summaries",
    "title": "EDA",
    "section": "Graphical Summaries",
    "text": "Graphical Summaries\nTo begin, I analyzed the distribution of diabetes status by gender. While the dataset contains a higher number of women compared to men, the prevalence of diabetes appears to be similar between the two groups.\n\nset.seed(1234)\ntable(data$Sex,data$Diabetes_binary)\n\n        \n         Not_Diabetic Diabetic\n  Female       123563    18411\n  Male          94771    16935\n\nggplot(data=data,aes(x=Sex,fill=Diabetes_binary))+\n  geom_bar()+\n  labs(x=\"Sex\")+\n  scale_fill_discrete(\"Diagnosis\")+\n  coord_flip()"
  },
  {
    "objectID": "EDA.html#boxplots",
    "href": "EDA.html#boxplots",
    "title": "EDA",
    "section": "BoxPlots",
    "text": "BoxPlots\nNext, I explored the relationship between lifestyle factors, specifically daily vegetable consumption and physical activity, and their association with elevated BMI and the prevalence of diabetes. I focused on these behaviors rather than metrics such as high cholesterol or blood pressure, as my interest was in examining individual habits rather than physiological indicators. The analysis revealed that among participants with diabetes who reported not consuming vegetables daily, approximately 75% had a BMI exceeding the median BMI of those who consumed vegetables daily and were not diabetic.\n\nset.seed(1234)\n# Create a grouped boxplot using ggplot2\nggplot(data = data, aes(x = Veggies, y = BMI,\n                        fill = factor(Diabetes_binary))) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(fill = \"Diabetes Status\")"
  },
  {
    "objectID": "Modeling.html#reading-in-data-and-converting-variables.",
    "href": "Modeling.html#reading-in-data-and-converting-variables.",
    "title": "Modeling",
    "section": "Reading in Data and Converting Variables.",
    "text": "Reading in Data and Converting Variables.\n\ndata&lt;-read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndata&lt;- data|&gt;\n  mutate(HighBP=factor(HighBP, levels=c(0,1),labels=c(\"Normal_BP\",\"High_BP\")),\n         HighChol=factor(HighChol, levels=c(0,1),labels=c(\"Normal_Chol\",\"High_Chol\")),\n         CholCheck=as.factor(CholCheck),\n         Smoker=factor(Smoker,levels=c(0,1),labels=c(\"Non_Smoker\",\"Smoker\")),\n         Stroke=factor(Stroke, levels=c(0,1),labels=c(\"No_Stroke\",\"Yes_Stroke\")),\n         Fruits=factor(Fruits,levels=c(0,1),labels=c(\"No_Fruits\",\"Eats_Fruits\")),\n         Veggies=factor(Veggies,levels=c(0,1),labels=c(\"No_Veggies\",\"Eats_Veggies\")),\n         Sex=factor(Sex, level=c(0,1), labels = c(\"Female\",\"Male\")),\n         GenHlth=factor(GenHlth,levels=c(1,2,3,4,5), \n                        labels=c(\"Excellent\",\"Very_Good\",\"Good\",\"Fair\",\"Poor\")),\n         Age=factor(Age, levels=c(1,2,3,4,5,6,7,8,9,10,11,12,13),\n                    labels=c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\n                            \"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80+\" )),\n         Education=factor(Education,levels=c(1,2,3,4,5,6),labels=c(\"None-K\",\"Grades 1-8\",\n                                                                   \"Grades 9-11\",\"HS Diploma\",\"Some College\",\"Bachelor's Degree or Higher\")),\n         Income=factor(Income,levels=c(1,2,3,4,5,6,7,8),labels=c(\"&lt;10K\",\"&lt;15K\",\n          \"&lt;20K\",\"&lt;25K\",\"&lt;35K\",\"&lt;50K\",\n          \"&lt;75K\",\"&gt;=75K\")),\n         Diabetes_binary=factor(Diabetes_binary,levels=c(0,1),labels=c(\"Not_Diabetic\",\"Diabetic\")),\n         HeartDiseaseorAttack=factor(HeartDiseaseorAttack,levels=c(0,1),labels=c(\"No_Heart Disease\",\"Heart_Disease\")),\n         PhysActivity=factor(PhysActivity,levels=c(0,1),labels=c(\"No_Physical Activity\",\"Physical_Activity\")),\n         HvyAlcoholConsump=factor(HvyAlcoholConsump,levels=c(0,1),labels=c(\"Not_Heavy Drinker\",\"Heavy_Drinker\")),\n         AnyHealthcare=factor(AnyHealthcare,levels=c(0,1),labels=c(\"No_Health_Coverage\",\"Yes_Health_Coverage\")),\n         NoDocbcCost=factor(NoDocbcCost,levels=c(0,1),labels=c(\"Cost_Issue\",\"Cost_Not_Issue\")),\n         DiffWalk=factor(DiffWalk,levels=c(0,1),labels=c(\"No_Difficulty_Walking\",\"Difficulty_Walking\"))\n         )"
  },
  {
    "objectID": "Modeling.html#creating-training-data",
    "href": "Modeling.html#creating-training-data",
    "title": "Modeling",
    "section": "Creating Training Data",
    "text": "Creating Training Data\nOn this modeling page, I will utilize the same dataset explored on the EDA page to fit a Classification Tree and a Random Forest model. The data will be split into two subsets: 70% for training and 30% for testing. Additionally, I will implement 5-fold cross-validation on the training dataset. This cross-validation procedure partitions the training data into five subsets, where each subset is used as a validation set once while the remaining subsets are used for training. This approach ensures robust resampling and improves the reliability of our model evaluation.\n\nset.seed(1234)\n\nsplit_data&lt;-initial_split(data,prop=0.7,strata = Diabetes_binary)\ntrain_data&lt;-training(split_data)\ntest_data&lt;-testing(split_data)\ncv_fold_data&lt;-vfold_cv(train_data,v=5,strata=Diabetes_binary)"
  },
  {
    "objectID": "Modeling.html#classification-tree-model",
    "href": "Modeling.html#classification-tree-model",
    "title": "Modeling",
    "section": "Classification Tree Model",
    "text": "Classification Tree Model\nIn this section, I will train a Classification Tree model. This model predicts a binary outcome—whether an individual has diabetes—based on predictor variables such as general health, diet, exercise, gender, and health insurance. The algorithm identifies the optimal root node and leaf nodes by learning patterns within the predictor variables that map to the correct output labels. It achieves this while minimizing a loss function that quantifies prediction errors. Classification Trees are particularly advantageous because they do not rely on strict assumptions about the data, such as normality or constant variance, making them flexible for various datasets. Additionally, their structure is straightforward and easy to interpret, enhancing their utility in practical applications.\n\nset.seed(1234)\ndata2&lt;-data|&gt;\n  select(Sex,Veggies,AnyHealthcare,GenHlth,PhysActivity)\n\n# Creating a recipe \nrec1&lt;-recipe(Diabetes_binary~Sex+Veggies+AnyHealthcare+GenHlth+PhysActivity,data=train_data)|&gt;\n   step_dummy(Sex,Veggies,GenHlth,AnyHealthcare,PhysActivity)\n\n\n#Creating a decision tree allowing for multiple depths with a minimum of 5 trees. \ntree_mod&lt;-decision_tree(tree_depth = tune(),\n                        min_n = 5,\n                        cost_complexity = tune()\n                        )|&gt;\n  set_engine(\"rpart\")|&gt;\n  set_mode(\"classification\")\n\n\n#Creating the workflow that uses the model and recipe set above. \ntree_wkf&lt;-workflow()|&gt;\n  add_recipe(rec1)|&gt;\n  add_model(tree_mod)\n\n\n#Training the model with the cross validation data set using a log loss metric.\ntemp&lt;-tree_wkf|&gt;\n  tune_grid(resamples = cv_fold_data,metrics = metric_set(mn_log_loss))\n  temp|&gt;\n    collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        2.90e- 8          6 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2        1.29e- 4          9 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3        4.49e- 6          2 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4        2.52e- 4         10 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5        1.95e- 3         14 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6        1.03e- 7         12 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 7        4.64e- 9          8 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 8        1.02e- 6          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 9        3.53e- 2          5 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10        1.90e-10         11 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n\n\n\n  tree_grid&lt;-grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels=c(5,5))\n\n\n  tree_fits&lt;-tree_wkf|&gt;\n    tune_grid(resamples=cv_fold_data,metrics=metric_set(mn_log_loss),\n              grid=tree_grid)\n\n\n#Listing out the the results \n  tree_fits|&gt;\n    collect_metrics()|&gt;\n    filter(.metric==\"mn_log_loss\")|&gt;\n    arrange(mean)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10    0.1                   4 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n# ℹ 15 more rows\n\n\n\n#Selecting the Best Model\ntree_best_params&lt;-select_best(tree_fits, metric=\"mn_log_loss\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          1 Preprocessor1_Model01\n\n\n\n# Refitting the workflow \ntree_final_wkf&lt;-tree_wkf|&gt;\n  finalize_workflow(tree_best_params)\n\n\n# Finding the best fit.\ntree_final_fit&lt;-tree_final_wkf|&gt;\n  last_fit(split_data,metrics=metric_set(mn_log_loss))\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177575/76105]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\n\n# Finding the best model \ntree_final_fit|&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.404 Preprocessor1_Model1\n\n\n\n#Extracting the model.\ntree_final_model&lt;-extract_workflow(tree_final_fit)\ntree_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 177575 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 177575 24742 Not_Diabetic (0.8606673 0.1393327) *\n\n\n\n# Here we can see the decision tree. \ntree_final_model %&gt;%\n  extract_fit_engine () %&gt;%\n  rpart.plot::rpart.plot(roundint=FALSE)"
  },
  {
    "objectID": "Modeling.html#log-loss-metric-for-decision-tree",
    "href": "Modeling.html#log-loss-metric-for-decision-tree",
    "title": "Modeling",
    "section": "Log Loss Metric for Decision Tree",
    "text": "Log Loss Metric for Decision Tree\n\ntree_final_fit|&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.404 Preprocessor1_Model1"
  },
  {
    "objectID": "Modeling.html#log-loss-metric-for-random-forest",
    "href": "Modeling.html#log-loss-metric-for-random-forest",
    "title": "Modeling",
    "section": "Log Loss Metric for Random Forest",
    "text": "Log Loss Metric for Random Forest\n\nrf_final_fit|&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.357 Preprocessor1_Model1"
  },
  {
    "objectID": "Modeling.html#determining-how-people-the-best-model-with-the-original-data-set-would-predict-to-be-diabetic-and-not.",
    "href": "Modeling.html#determining-how-people-the-best-model-with-the-original-data-set-would-predict-to-be-diabetic-and-not.",
    "title": "Modeling",
    "section": "Determining how people the best model with the original data set would predict to be diabetic and not.",
    "text": "Determining how people the best model with the original data set would predict to be diabetic and not.\n\npredictions &lt;- predict(best, data2)\n\n\n  class_table &lt;- table(predictions)\n  \n  class_table\n\n.pred_class\nNot_Diabetic     Diabetic \n      253680            0 \n\n\n\n  prevalent_class &lt;- names(which.max(class_table))\n  \n\n  list(predicted_class = prevalent_class)\n\n$predicted_class\n[1] \"Not_Diabetic\""
  }
]